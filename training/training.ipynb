{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as td\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters and settings\n",
    "batch_size = 64\n",
    "test_batch_size = 64\n",
    "input_size = 1 # because there is only one channel \n",
    "output_size = 4\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load traiing, validation and training data\n",
    "\n",
    "data_loader = torch.load('data_loader.pt')\n",
    "valid_loader = torch.load('valid_loader.pt')\n",
    "test_loader = torch.load('test_loader.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        self.name = \"CNN\"\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv_layer = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.fc_layer = nn.Sequential(\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(12 * 12 * 64, 1000),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1000, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        # conv layers\n",
    "        x = self.conv_layer(x)\n",
    "        # print(x.shape)\n",
    "        # flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # print(x.shape)\n",
    "        # fc layer\n",
    "        x = self.fc_layer(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN2, self).__init__()\n",
    "        self.name = \"CNN2\"\n",
    "        self.conv_layer = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=5, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=5, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=5, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            #get dimensions of last layer\n",
    "            \n",
    "        )\n",
    "        self.fc_layer = nn.Sequential(\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(64*9*9, 32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(32, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        # conv layers\n",
    "        x = self.conv_layer(x)\n",
    "        # print(x.shape)\n",
    "        # flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # print(x.shape)\n",
    "        # fc layer\n",
    "        x = self.fc_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN3, self).__init__()\n",
    "        self.name = \"CNN3\"\n",
    "        self.conv_layer = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=64, kernel_size=7, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=7, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "           \n",
    "            \n",
    "        )\n",
    "        \n",
    "        self.fc_layer = nn.Sequential(\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(20*20*64, 32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(32, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        # conv layers\n",
    "        x = self.conv_layer(x)\n",
    "        # print(x.shape)\n",
    "        # flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # print(x.shape)\n",
    "        # fc layer\n",
    "        x = self.fc_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_model(model, data_loader, valid_loader):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    total_step = len(data_loader)\n",
    "    loss_list = []\n",
    "    acc_list = []\n",
    "\n",
    "    loss_list_test = []\n",
    "    acc_list_test = []\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (images, labels) in enumerate(data_loader):\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss_list.append(loss.item())\n",
    "            # Backprop and optimisation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # Train accuracy\n",
    "            total = labels.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct = (predicted == labels).sum().item()\n",
    "            acc_list.append(correct / total)\n",
    "            # print(i)\n",
    "            # if (i + 1) % 10 == 0:\n",
    "            #     print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%'\n",
    "            #     .format(epoch + 1, num_epochs, i + 1, total_step, loss.item(),\n",
    "            #     (correct / total) * 100))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i, (images_test, labels_test) in enumerate(valid_loader):\n",
    "                outputs_test = model(images_test)\n",
    "                loss_test = criterion(outputs_test, labels_test)\n",
    "                loss_list_test.append(loss_test.item())\n",
    "                total_test = labels_test.size(0)\n",
    "                _, predicted_test = torch.max(outputs_test.data, 1)\n",
    "                correct_test = (predicted_test == labels_test).sum().item()\n",
    "                acc_list_test.append(correct_test / total_test)\n",
    "        print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%'\n",
    "        .format(epoch + 1, num_epochs, i + 1, total_step, loss_test.item(),\n",
    "        (correct / total) * 100))\n",
    "        \n",
    "        print(loss_test.item())\n",
    "        \n",
    "\n",
    "    # Save model to file\n",
    "    torch.save(model.state_dict(), '%s.ckpt'%(model.name))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [10/44], Loss: 0.9008, Accuracy: 66.67%\n",
      "0.9008370041847229\n",
      "Epoch [2/10], Step [10/44], Loss: 0.5256, Accuracy: 83.33%\n",
      "0.5256298780441284\n",
      "Epoch [3/10], Step [10/44], Loss: 1.1318, Accuracy: 79.17%\n",
      "1.1317824125289917\n",
      "Epoch [4/10], Step [10/44], Loss: 0.1839, Accuracy: 83.33%\n",
      "0.18392497301101685\n",
      "Epoch [5/10], Step [10/44], Loss: 0.5536, Accuracy: 95.83%\n",
      "0.553557813167572\n",
      "Epoch [6/10], Step [10/44], Loss: 0.3379, Accuracy: 87.50%\n",
      "0.3379003703594208\n",
      "Epoch [7/10], Step [10/44], Loss: 0.5953, Accuracy: 91.67%\n",
      "0.5953469276428223\n",
      "Epoch [8/10], Step [10/44], Loss: 0.3537, Accuracy: 95.83%\n",
      "0.3537135124206543\n",
      "Epoch [9/10], Step [10/44], Loss: 0.0593, Accuracy: 100.00%\n",
      "0.0592837929725647\n",
      "Epoch [10/10], Step [10/44], Loss: 0.3104, Accuracy: 87.50%\n",
      "0.3104085624217987\n"
     ]
    }
   ],
   "source": [
    "model1 = CNN()\n",
    "train_and_save_model(model1, data_loader, valid_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [10/44], Loss: 0.3491, Accuracy: 91.67%\n",
      "0.3491349518299103\n",
      "Epoch [2/10], Step [10/44], Loss: 0.6591, Accuracy: 95.83%\n",
      "0.6590884327888489\n",
      "Epoch [3/10], Step [10/44], Loss: 0.3459, Accuracy: 87.50%\n",
      "0.34587791562080383\n",
      "Epoch [4/10], Step [10/44], Loss: 0.0287, Accuracy: 75.00%\n",
      "0.028725184500217438\n",
      "Epoch [5/10], Step [10/44], Loss: 0.3504, Accuracy: 95.83%\n",
      "0.3503977060317993\n",
      "Epoch [6/10], Step [10/44], Loss: 0.0724, Accuracy: 91.67%\n",
      "0.07237362116575241\n",
      "Epoch [7/10], Step [10/44], Loss: 0.3712, Accuracy: 95.83%\n",
      "0.37121155858039856\n",
      "Epoch [8/10], Step [10/44], Loss: 0.0116, Accuracy: 95.83%\n",
      "0.011619550175964832\n",
      "Epoch [9/10], Step [10/44], Loss: 0.2681, Accuracy: 91.67%\n",
      "0.26814743876457214\n",
      "Epoch [10/10], Step [10/44], Loss: 0.2143, Accuracy: 91.67%\n",
      "0.21433930099010468\n"
     ]
    }
   ],
   "source": [
    "model2 = CNN2()\n",
    "train_and_save_model(model2, data_loader, valid_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [10/44], Loss: 0.6144, Accuracy: 83.33%\n",
      "0.6143677830696106\n",
      "Epoch [2/10], Step [10/44], Loss: 0.4271, Accuracy: 75.00%\n",
      "0.4270746409893036\n",
      "Epoch [3/10], Step [10/44], Loss: 0.1920, Accuracy: 95.83%\n",
      "0.19197624921798706\n",
      "Epoch [4/10], Step [10/44], Loss: 0.4687, Accuracy: 95.83%\n",
      "0.4686531722545624\n",
      "Epoch [5/10], Step [10/44], Loss: 0.2541, Accuracy: 91.67%\n",
      "0.2540655732154846\n",
      "Epoch [6/10], Step [10/44], Loss: 0.5060, Accuracy: 87.50%\n",
      "0.5059526562690735\n",
      "Epoch [7/10], Step [10/44], Loss: 0.9441, Accuracy: 95.83%\n",
      "0.9441371560096741\n",
      "Epoch [8/10], Step [10/44], Loss: 0.4995, Accuracy: 95.83%\n",
      "0.49953651428222656\n",
      "Epoch [9/10], Step [10/44], Loss: 0.1159, Accuracy: 91.67%\n",
      "0.1159004345536232\n",
      "Epoch [10/10], Step [10/44], Loss: 2.3058, Accuracy: 95.83%\n",
      "2.305830240249634\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model3 = CNN3()\n",
    "train_and_save_model(model3, data_loader, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, valid_loader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in valid_loader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        print('Test Accuracy of the model (validation): {} %'\n",
    "        .format((correct / total) * 100))\n",
    "\n",
    "def test_model(model, test_loader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in test_loader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        print('Test Accuracy of the model (testing): {} %'\n",
    "        .format((correct / total) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model (validation): 88.66666666666667 %\n",
      "Test Accuracy of the model (testing): 89.66666666666666 %\n",
      "Test Accuracy of the model (validation): 92.33333333333333 %\n",
      "Test Accuracy of the model (testing): 92.66666666666666 %\n",
      "Test Accuracy of the model (validation): 86.33333333333333 %\n",
      "Test Accuracy of the model (testing): 89.0 %\n"
     ]
    }
   ],
   "source": [
    "for model in [model1, model2, model3]:\n",
    "    evaluate_model(model, valid_loader)\n",
    "    test_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to test an individual image from an external source\n",
    "def test_individual_image(model, image_name, category, read_custom_path = ''):\n",
    "    if (read_custom_path != ''):\n",
    "        img = cv2.imread(read_custom_path, 0)\n",
    "        image_name = read_custom_path\n",
    "        #save the image\n",
    "    else:\n",
    "        img = cv2.imread(\"../concat_data/%s/%s\" % (category, image_name), 0)\n",
    "    \n",
    "    #resize to 48x48 pixels\n",
    "    img = cv2.resize(img, (48, 48))\n",
    "\n",
    "\n",
    "\n",
    "    #if image 3 channel convert to 1 channel\n",
    "    if len(img.shape) > 2:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    cv2.imwrite(\"image_mod.jpg\", img)\n",
    "    \n",
    "    # Convert image to tensor and add batch and channel dimensions\n",
    "    img_tensor = torch.tensor(img, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
    "    \n",
    "    # Get label tensor\n",
    "    labels = {'focused': 0, 'happy': 1, 'neutral': 2, 'surprised': 3}\n",
    "    reverse_labels = {0: 'focused', 1: 'happy', 2: 'neutral', 3: 'surprised'}\n",
    "    label_tensor = torch.tensor(labels[category], dtype=torch.long)\n",
    "\n",
    "    # Forward pass for the single image\n",
    "    output = model(img_tensor)\n",
    "    _, predicted = torch.max(output, 1)\n",
    "\n",
    "    # Print results\n",
    "    print(\"Predicted:\", predicted.item(), \"(\", reverse_labels[predicted.item()], \")\")\n",
    "    print(\"Actual:\", label_tensor.item(), \"(\", reverse_labels[label_tensor.item()], \")\")\n",
    "    print(\"Image:\", image_name)\n",
    "    print(\"Category:\", category)\n",
    "    print(\"///////\")\n",
    "    \n",
    "    return predicted.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: 1 ( happy )\n",
      "Actual: 1 ( happy )\n",
      "Image: C:\\Users\\Luis\\Downloads\\test_smile.PNG\n",
      "Category: happy\n",
      "///////\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_individual_image(model1, \"86_MMA-FACIAL-EXPRESSION-mahmoud.jpg\", \"happy\", read_custom_path=r\"C:\\Users\\Luis\\Downloads\\test_smile.PNG\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: 1 ( happy )\n",
      "Actual: 2 ( neutral )\n",
      "Image: C:\\Users\\Luis\\Downloads\\test_neutral.PNG\n",
      "Category: neutral\n",
      "///////\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_individual_image(model1, \"86_MMA-FACIAL-EXPRESSION-mahmoud.jpg\", \"neutral\", read_custom_path=r\"C:\\Users\\Luis\\Downloads\\test_neutral.PNG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: 3 ( surprised )\n",
      "Actual: 0 ( focused )\n",
      "Image: C:\\Users\\Luis\\Downloads\\test_focused.PNG\n",
      "Category: focused\n",
      "///////\n",
      "Predicted: 3 ( surprised )\n",
      "Actual: 0 ( focused )\n",
      "Image: C:\\Users\\Luis\\Downloads\\test_focused_2.PNG\n",
      "Category: focused\n",
      "///////\n",
      "Predicted: 0 ( focused )\n",
      "Actual: 0 ( focused )\n",
      "Image: C:\\Users\\Luis\\Downloads\\test_focused_3.PNG\n",
      "Category: focused\n",
      "///////\n",
      "Predicted: 1 ( happy )\n",
      "Actual: 0 ( focused )\n",
      "Image: C:\\Users\\Luis\\Downloads\\test_focused_4.PNG\n",
      "Category: focused\n",
      "///////\n",
      "Predicted: 0 ( focused )\n",
      "Actual: 0 ( focused )\n",
      "Image: C:\\Users\\Luis\\Downloads\\test_focused_5.PNG\n",
      "Category: focused\n",
      "///////\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_individual_image(model1, \"86_MMA-FACIAL-EXPRESSION-mahmoud.jpg\", \"focused\", read_custom_path=r\"C:\\Users\\Luis\\Downloads\\test_focused.PNG\")\n",
    "test_individual_image(model1, \"86_MMA-FACIAL-EXPRESSION-mahmoud.jpg\", \"focused\", read_custom_path=r\"C:\\Users\\Luis\\Downloads\\test_focused_2.PNG\")\n",
    "test_individual_image(model1, \"86_MMA-FACIAL-EXPRESSION-mahmoud.jpg\", \"focused\", read_custom_path=r\"C:\\Users\\Luis\\Downloads\\test_focused_3.PNG\")\n",
    "test_individual_image(model1, \"86_MMA-FACIAL-EXPRESSION-mahmoud.jpg\", \"focused\", read_custom_path=r\"C:\\Users\\Luis\\Downloads\\test_focused_4.PNG\")\n",
    "test_individual_image(model1, \"86_MMA-FACIAL-EXPRESSION-mahmoud.jpg\", \"focused\", read_custom_path=r\"C:\\Users\\Luis\\Downloads\\test_focused_5.PNG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: 3 ( surprised )\n",
      "Actual: 3 ( surprised )\n",
      "Image: C:\\Users\\Luis\\Downloads\\test_surprised.PNG\n",
      "Category: surprised\n",
      "///////\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_individual_image(model1, \"86_MMA-FACIAL-EXPRESSION-mahmoud.jpg\", \"surprised\", read_custom_path=r\"C:\\Users\\Luis\\Downloads\\test_surprised.PNG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: 0 ( focused )\n",
      "Actual: 0 ( focused )\n",
      "Image: 86_MMA-FACIAL-EXPRESSION-mahmoud.jpg\n",
      "Category: focused\n",
      "///////\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_individual_image(model1, \"86_MMA-FACIAL-EXPRESSION-mahmoud.jpg\", \"focused\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
