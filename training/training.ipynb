{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as td\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Takes images in concat_data and flatten them into a 1D array\n",
    "def flatten_images(category):\n",
    "    #Get the list of images in the directory\n",
    "    images = os.listdir(\"../concat_data/%s\" % category)\n",
    "    #Create an empty list to store the flattened images\n",
    "    flattened_images = []\n",
    "    #Iterate through the images\n",
    "    for image in images:\n",
    "        #check extension of the image\n",
    "        if image.split(\".\")[-1] == \"jpg\":\n",
    "            #Read the image\n",
    "            img = cv2.imread(\"../concat_data/%s/%s\" %(category, image), 0)\n",
    "            #Flatten the image\n",
    "            # img = img.flatten()\n",
    "            #Add the flattened image to the list\n",
    "            flattened_images.append(img)\n",
    "\n",
    "    #Return the list of flattened images\n",
    "    return flattened_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add images of each category into an array\n",
    "\n",
    "\n",
    "images_dict = {'focused': flatten_images('focused'),\n",
    "               'happy': flatten_images('happy'),\n",
    "               'neutral': flatten_images('neutral'),\n",
    "               'surprised': flatten_images('surprised')}\n",
    "\n",
    "# tokensize labels\n",
    "labels = {'focused': 0,\n",
    "          'happy': 1,\n",
    "          'neutral': 2,\n",
    "          'surprised': 3}\n",
    "\n",
    "\n",
    "# concatencate all the data with respective labels\n",
    "x = []\n",
    "y = []\n",
    "for key in images_dict:\n",
    "    for image in images_dict[key]:\n",
    "        x.append(image)\n",
    "        y.append(labels[key])\n",
    "        \n",
    "# split into training, validation, and testing\n",
    "x_train, x_temp, y_train, y_temp = train_test_split(x, y, test_size =0.30, random_state=42)\n",
    "\n",
    "# split x_temp and y_temp into validation and testing\\\n",
    "x_valid, x_test, y_valid, y_test = train_test_split(x_temp, y_temp, test_size =0.50, random_state=42)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all the data into tensors\n",
    "x_train_tensor = torch.tensor(x_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train)\n",
    "\n",
    "x_valid_tensor = torch.tensor(x_valid, dtype=torch.float32)\n",
    "y_valid_tensor = torch.tensor(y_valid)\n",
    "\n",
    "x_test_tensor = torch.tensor(x_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters and settings\n",
    "batch_size = 64\n",
    "test_batch_size = 64\n",
    "input_size = 1 # because there is only one channel \n",
    "output_size = 4\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 27.,  15.,  18.,  ...,  28.,  53.,  33.],\n",
      "        [ 31.,  31.,  29.,  ...,  23.,  22.,  23.],\n",
      "        [ 41.,  41.,  40.,  ...,  29.,  26.,  31.],\n",
      "        ...,\n",
      "        [254., 253., 252.,  ..., 149., 148., 153.],\n",
      "        [251., 255., 253.,  ..., 154., 156., 149.],\n",
      "        [254., 251., 239.,  ..., 164., 152., 141.]])\n"
     ]
    }
   ],
   "source": [
    "# x_numpy = np.array(x)/255.0 # Normalized values \n",
    "# y_numpy = np.array(y)\n",
    "# x_reshaped = x_numpy.reshape(len(x_numpy), 1, 48, 48)\n",
    "# # print(x_reshaped)\n",
    "# # Convert numpy arrays to PyTorch tensors\n",
    "# images_tensor = torch.tensor(x_numpy, dtype=torch.float32)\n",
    "# images_tensor = images_tensor.unsqueeze(1)\n",
    "# labels_tensor = torch.tensor(y_numpy, dtype=torch.long)\n",
    "images_tensor = torch.tensor(x_train, dtype=torch.float32)\n",
    "images_tensor = images_tensor.unsqueeze(1)\n",
    "labels_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "\n",
    "images_test_tensor = torch.tensor(x_valid, dtype=torch.float32)\n",
    "images_test_tensor = images_test_tensor.unsqueeze(1)\n",
    "labels_test_tensor = torch.tensor(y_valid, dtype=torch.long)\n",
    "\n",
    "print(images_tensor[3][0])\n",
    "# Create a TensorDataset\n",
    "dataset = td.TensorDataset(images_tensor, labels_tensor)\n",
    "\n",
    "# Create DataLoader for batching and shuffling\n",
    "batch_size = 32\n",
    "data_loader = td.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = td.DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv_layer = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.fc_layer = nn.Sequential(\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(12 * 12 * 64, 1000),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1000, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        # conv layers\n",
    "        x = self.conv_layer(x)\n",
    "        # print(x.shape)\n",
    "        # flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # print(x.shape)\n",
    "        # fc layer\n",
    "        x = self.fc_layer(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1400\n"
     ]
    }
   ],
   "source": [
    "print(len(x_train_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_step = len(data_loader)\n",
    "loss_list = []\n",
    "acc_list = []\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(data_loader):\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss_list.append(loss.item())\n",
    "        # Backprop and optimisation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # Train accuracy\n",
    "        total = labels.size(0)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct = (predicted == labels).sum().item()\n",
    "        acc_list.append(correct / total)\n",
    "        # print(i)\n",
    "        if (i + 1) % 62 == 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%'\n",
    "            .format(epoch + 1, num_epochs, i + 1, total_step, loss.item(),\n",
    "            (correct / total) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the 10000 test images: 100.0 %\n",
      "Test Accuracy of the model on the 10000 test images: 98.4375 %\n",
      "Test Accuracy of the model on the 10000 test images: 98.95833333333334 %\n",
      "Test Accuracy of the model on the 10000 test images: 99.21875 %\n",
      "Test Accuracy of the model on the 10000 test images: 99.375 %\n",
      "Test Accuracy of the model on the 10000 test images: 98.95833333333334 %\n",
      "Test Accuracy of the model on the 10000 test images: 99.10714285714286 %\n",
      "Test Accuracy of the model on the 10000 test images: 99.21875 %\n",
      "Test Accuracy of the model on the 10000 test images: 99.30555555555556 %\n",
      "Test Accuracy of the model on the 10000 test images: 99.0625 %\n",
      "Test Accuracy of the model on the 10000 test images: 99.14772727272727 %\n",
      "Test Accuracy of the model on the 10000 test images: 98.95833333333334 %\n",
      "Test Accuracy of the model on the 10000 test images: 98.79807692307693 %\n",
      "Test Accuracy of the model on the 10000 test images: 98.66071428571429 %\n",
      "Test Accuracy of the model on the 10000 test images: 98.75 %\n",
      "Test Accuracy of the model on the 10000 test images: 98.828125 %\n",
      "Test Accuracy of the model on the 10000 test images: 98.89705882352942 %\n",
      "Test Accuracy of the model on the 10000 test images: 98.95833333333334 %\n",
      "Test Accuracy of the model on the 10000 test images: 98.84868421052632 %\n",
      "Test Accuracy of the model on the 10000 test images: 98.90625 %\n",
      "Test Accuracy of the model on the 10000 test images: 98.95833333333334 %\n",
      "Test Accuracy of the model on the 10000 test images: 98.86363636363636 %\n",
      "Test Accuracy of the model on the 10000 test images: 98.91304347826086 %\n",
      "Test Accuracy of the model on the 10000 test images: 98.95833333333334 %\n",
      "Test Accuracy of the model on the 10000 test images: 98.875 %\n",
      "Test Accuracy of the model on the 10000 test images: 98.79807692307693 %\n",
      "Test Accuracy of the model on the 10000 test images: 98.72685185185185 %\n",
      "Test Accuracy of the model on the 10000 test images: 98.77232142857143 %\n",
      "Test Accuracy of the model on the 10000 test images: 98.8146551724138 %\n",
      "Test Accuracy of the model on the 10000 test images: 98.85416666666667 %\n",
      "Test Accuracy of the model on the 10000 test images: 98.68951612903226 %\n",
      "Test Accuracy of the model on the 10000 test images: 98.73046875 %\n",
      "Test Accuracy of the model on the 10000 test images: 98.67424242424242 %\n",
      "Test Accuracy of the model on the 10000 test images: 98.71323529411765 %\n",
      "Test Accuracy of the model on the 10000 test images: 98.66071428571429 %\n",
      "Test Accuracy of the model on the 10000 test images: 98.61111111111111 %\n",
      "Test Accuracy of the model on the 10000 test images: 98.5641891891892 %\n",
      "Test Accuracy of the model on the 10000 test images: 98.60197368421053 %\n",
      "Test Accuracy of the model on the 10000 test images: 98.4775641025641 %\n",
      "Test Accuracy of the model on the 10000 test images: 98.4375 %\n",
      "Test Accuracy of the model on the 10000 test images: 98.47560975609755 %\n",
      "Test Accuracy of the model on the 10000 test images: 98.51190476190477 %\n",
      "Test Accuracy of the model on the 10000 test images: 98.54651162790698 %\n",
      "Test Accuracy of the model on the 10000 test images: 98.57142857142858 %\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        print('Test Accuracy of the model on the 10000 test images: {} %'\n",
    "        .format((correct / total) * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
