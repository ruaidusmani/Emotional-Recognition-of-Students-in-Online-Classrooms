{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as td\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Takes images in concat_data and flatten them into a 1D array\n",
    "def flatten_images():\n",
    "    images_dict = {'focused': [],\n",
    "                    'happy': [],\n",
    "                    'neutral': [],\n",
    "                    'surprised': []}\n",
    "    image_name_dict = {'focused': [],\n",
    "                    'happy': [],\n",
    "                    'neutral': [],\n",
    "                    'surprised': []}\n",
    "    \n",
    "    for category in ['focused', 'happy', 'neutral', 'surprised']:\n",
    "        #Get the list of images in the directory\n",
    "        images = os.listdir(\"../concat_data/%s\" % category)\n",
    "        #Create an empty list to store the flattened images\n",
    "        flattened_images = []\n",
    "        #Iterate through the images\n",
    "        \n",
    "        for image in images:\n",
    "            #check extension of the image\n",
    "            if image.split(\".\")[-1] == \"jpg\":\n",
    "                #Read the image\n",
    "                img = cv2.imread(\"../concat_data/%s/%s\" %(category, image), 0)\n",
    "                #Flatten the image\n",
    "                # img = img.flatten()\n",
    "                #Add the flattened image to the list\n",
    "                images_dict[category].append(img)\n",
    "                image_name_dict[category].append(category + \"_\" + image)\n",
    "\n",
    "\n",
    "    #Return the list of flattened images\n",
    "    return images_dict, image_name_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add images of each category into an array\n",
    "\n",
    "\n",
    "images_dict, image_name_dict = flatten_images() \n",
    "\n",
    "# tokensize labels\n",
    "labels = {'focused': 0,\n",
    "          'happy': 1,\n",
    "          'neutral': 2,\n",
    "          'surprised': 3}\n",
    "\n",
    "\n",
    "# concatencate all the data with respective labels\n",
    "x = []\n",
    "y = []\n",
    "for key in images_dict:\n",
    "    for image in images_dict[key]:\n",
    "        x.append(image)\n",
    "        y.append(labels[key])\n",
    "x_name = []\n",
    "y_name = []\n",
    "for key in image_name_dict:\n",
    "    for image_name in image_name_dict[key]:\n",
    "        x_name.append(image_name)\n",
    "        y_name.append(labels[key])\n",
    "        \n",
    "# split into training and valid/testing\n",
    "x_train, x_temp, y_train, y_temp = train_test_split(x, y, test_size =0.30, random_state=42)\n",
    "x_train_name, x_temp_name, y_train_name, y_temp_name = train_test_split(x_name, y_name, test_size =0.30, random_state=42)\n",
    "\n",
    "# split x_temp and y_temp into validation and testing\\\n",
    "x_valid, x_test, y_valid, y_test = train_test_split(x_temp, y_temp, test_size =0.50, random_state=42)\n",
    "x_valid_name, x_test_name, y_valid_name, y_test_name = train_test_split(x_temp_name, y_temp_name, test_size =0.50, random_state=42)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters and settings\n",
    "batch_size = 64\n",
    "test_batch_size = 64\n",
    "input_size = 1 # because there is only one channel \n",
    "output_size = 4\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_image(arr_in):\n",
    "    for directory in ['focused', 'happy', 'neutral', 'surprised']:\n",
    "                images = os.listdir(\"../concat_data/%s\" % directory)\n",
    "                #Iterate through the images\n",
    "                for image in images:\n",
    "                    #check extension of the image\n",
    "                    if image.split(\".\")[-1] == \"jpg\":\n",
    "                        #Read the image\n",
    "                        img = cv2.imread(\"../concat_data/%s/%s\" %(directory, image), 0)\n",
    "                        #Flatten the image\n",
    "                        # img = img.flatten()\n",
    "                        #Add the flattened image to the list\n",
    "                        if np.array_equal(img, arr_in):\n",
    "                            # \n",
    "                            print(image)\n",
    "                            print(directory)\n",
    "                            return image, directory\n",
    "                            \n",
    "         \n",
    "    return None\n",
    "def find_respective_images(tensor_in):\n",
    "    # finds the image in the one of the directories\n",
    "    # tensor_in: tensor\n",
    "    # return: string\n",
    "    dict_out = {'focused': [], 'happy': [], 'neutral': [], 'surprised': []}\n",
    "\n",
    "    #Get the list of images in the directory\n",
    "    for element in tensor_in:\n",
    "        for element_name in element:\n",
    "            numpy_tensor = element_name.numpy()\n",
    "            image, directory = find_image(numpy_tensor)\n",
    "            dict_out[directory].append(image)\n",
    "\n",
    "            \n",
    "    \n",
    "    # print(tensor_in.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Luis\\AppData\\Local\\Temp\\ipykernel_21624\\2792377710.py:7: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:278.)\n",
      "  images_tensor = torch.tensor(x_train, dtype=torch.float32, device=device)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Create data that can be fed into pytorch\n",
    "\n",
    "#getting device type\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#training\n",
    "images_tensor = torch.tensor(x_train, dtype=torch.float32, device=device)\n",
    "images_tensor = images_tensor.unsqueeze(1)\n",
    "labels_tensor = torch.tensor(y_train, dtype=torch.long, device=device)\n",
    "\n",
    "# validation\n",
    "images_valid_tensor = torch.tensor(x_valid, dtype=torch.float32, device=device)\n",
    "images_valid_tensor = images_valid_tensor.unsqueeze(1)\n",
    "labels_valid_tensor = torch.tensor(y_valid, dtype=torch.long, device=device)\n",
    "\n",
    "# testing\n",
    "images_testing_tensor = torch.tensor(x_test, dtype=torch.float32, device=device)\n",
    "images_testing_tensor = images_testing_tensor.unsqueeze(1)\n",
    "labels_testing_tensor = torch.tensor(y_test, dtype=torch.long, device=device)\n",
    "\n",
    "batch_size = 2048\n",
    "\n",
    "\n",
    "# Create a TensorDataset\n",
    "# training\n",
    "dataset_train = td.TensorDataset(images_tensor, labels_tensor)\n",
    "data_loader = td.DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "# validation\n",
    "dataset_valid = td.TensorDataset(images_valid_tensor, labels_valid_tensor)\n",
    "valid_loader = td.DataLoader(dataset_valid, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "# testing\n",
    "dataset_test = td.TensorDataset(images_testing_tensor, labels_testing_tensor)\n",
    "test_loader = td.DataLoader(dataset_test, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "# # save each dataset\n",
    "# torch.save(dataset_train, 'dataset_train.pt')\n",
    "# torch.save(dataset_valid, 'dataset_valid.pt')\n",
    "# torch.save(dataset_test, 'dataset_test.pt')\n",
    "\n",
    "#save loaders\n",
    "torch.save(data_loader, 'data_loader.pt')\n",
    "torch.save(valid_loader, 'valid_loader.pt')\n",
    "torch.save(test_loader, 'test_loader.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_unique_sets():\n",
    "    for name in x_test_name:\n",
    "        if (name in x_valid_name) or (name in x_train_name):\n",
    "            print(name)\n",
    "            print(\"error\")\n",
    "    for name in x_valid_name:\n",
    "        if (name in x_test_name) or (name in x_train_name):\n",
    "            print(name)\n",
    "            print(\"error\")\n",
    "    for name in x_train_name:\n",
    "        if (name in x_valid_name) or (name in x_test_name):\n",
    "            print(name)\n",
    "            print(\"error\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
